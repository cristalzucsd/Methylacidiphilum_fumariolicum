{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180057ea",
   "metadata": {},
   "source": [
    "## 2.4 Model Validation and application\n",
    "### 2.4.3 Heterotrophic metabolism is redox-limited\n",
    "\n",
    "This notebook recreates the analyses and figures from section 2.4.3. Flux sampling of the model[1] using Methane, Propane, Isopropanol and Acetone. This analysis is divided in two notebooks, one dedicated to performing the simulations (This) and other dedicted to plotting and data analysis.\n",
    "\n",
    "The notebook is divided in three sections:\n",
    "1. In the first section we pre-process the model to reduce computational burden of the simulations\n",
    "2. The second section runs independent sampling using OptGPSampler [1]\n",
    "3. Finally, for each simulation we calculate the PageRank score for each reactiong using Mass Flow Graphs calculated following the methods presentend in [2]\n",
    "\n",
    "[1] Megchelenbrink W, Huynen M, Marchiori E (2014) optGpSampler: An Improved Tool for Uniformly Sampling the Solution-Space of Genome-Scale Metabolic Networks. PLoS ONE 9(2): e86587. https://doi.org/10.1371/journal.pone.0086587\n",
    "\n",
    "[2] Beguerisse-Díaz, M., Bosque, G., Oyarzún, D. et al. Flux-dependent graphs for metabolic networks. npj Syst Biol Appl 4, 32 (2018). https://doi.org/10.1038/s41540-018-0067-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb68408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the modeling environment\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from cobra.sampling import OptGPSampler\n",
    "from helpers import find_blocked_mets, build_normilized_flow_graph, build_mass_flow_graph\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "import cobra\n",
    "\n",
    "# Change solvers as available\n",
    "# Recomended solvers: 'cplex', 'gurobi'\n",
    "cobra_config = cobra.Configuration()\n",
    "cobra_config.solver = 'cplex'\n",
    "cobra_config.tolerance = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98befc",
   "metadata": {},
   "source": [
    "## Section 1 \n",
    "---\n",
    "\n",
    "Pre-processing of the model. This cell prints a list of metabolites and reactions removed from the model to reduce computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2495c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2hglut_c: SHGO, \n",
      "mmet_c: HCYSMT2, \n",
      "fe3_p: FE3abcpp, \n",
      "gsn_c: PUNP3, \n",
      "hg2_p: HG2tppi, \n",
      "4ahmmp_c: HMPK1, \n",
      "malthx_c: AMALT3, \n",
      "ncam_c: NNAM, \n",
      "n2_c: \n",
      "pmtcoa_c: FACOAE160, \n",
      "pyam5p_c: PYAM5PO, \n",
      "stcoa_c: FACOAE180, APG3PAT180, \n",
      "o2s_c: SPODM, \n",
      "xtsn_c: PUNP7, \n",
      "n2_p: \n",
      "n2_e: EX_n2_e, \n",
      "ch4s_c: MTOX, \n",
      "fe3_c: \n",
      "hg2_c: \n",
      "malt_c: AMALT2, AMALT1, MLTG1, \n",
      "xan_c: \n",
      "hemeA_c: HEMEAS_2, \n",
      "ditp_c: NTPP10, RNTRA5, RNTR5, \n",
      "cu_e: CUt3, \n",
      "ppoh_e: PPOHtex, \n",
      "cbl1_e: CBL1tex, \n",
      "cu_c: \n",
      "dimp_c: \n",
      "maltpt_c: \n",
      "maltttr_c: \n",
      "malttr_c: \n",
      "nac_c: NAPRT, \n",
      "cbl1_p: CBL1abcpp, \n",
      "cbl1_c: CBL1MT, \n",
      "FHL\n",
      "PO2OXG\n",
      "Kt2pp\n",
      "NAD_H2\n",
      "ZN2t3pp\n",
      "NAt3pp\n",
      "FE2t3pp\n",
      "PPANAp\n",
      "CU2t3pp\n",
      "MG2t3_2pp\n",
      "HYD4pp\n",
      "Kt3pp\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = cobra.io.read_sbml_model('../model_files/iMFP2023.xml')\n",
    "model.solver.configuration.lp_method = 'auto'\n",
    "\n",
    "# Remove unused reactions\n",
    "to_remove = ['N2tex', 'N2trpp', 'NIT1c']\n",
    "for id in to_remove:\n",
    "    rxn = model.reactions.get_by_id(id)\n",
    "    model.remove_reactions([rxn])\n",
    "\n",
    "# Identify and remove structuraly blocked metabolites and their associated reactions\n",
    "orphans, deadends = find_blocked_mets(model)\n",
    "\n",
    "blocked_mets = orphans.copy()\n",
    "blocked_mets.extend(deadends)\n",
    "blocked_mets.extend(['ditp_c', 'cu_e', 'ppoh_e', 'cbl1_e'])\n",
    "blocked_mets = list(pd.unique(blocked_mets))\n",
    "\n",
    "while blocked_mets:\n",
    "    for id in blocked_mets:\n",
    "        \n",
    "        met = model.metabolites.get_by_id(id)\n",
    "        print(f\"{id}: \", end=\"\")\n",
    "        \n",
    "        for rxn in met.reactions:\n",
    "            \n",
    "            model.remove_reactions([rxn])\n",
    "            print(f\"{rxn.id}, \", end=\"\")\n",
    "            \n",
    "        model.remove_metabolites([met])\n",
    "        print(\"\")\n",
    "        \n",
    "    orphans, deadends = find_blocked_mets(model)\n",
    "    \n",
    "    blocked_mets = orphans.copy()\n",
    "    blocked_mets.extend(deadends)\n",
    "    blocked_mets = list(pd.unique(blocked_mets))\n",
    "\n",
    "# remove constrained reactions\n",
    "for rxn in model.reactions:\n",
    "    lb = rxn.lower_bound\n",
    "    ub = rxn.upper_bound\n",
    "    if lb == ub:\n",
    "        model.remove_reactions([rxn])\n",
    "        print(rxn.id)\n",
    "        \n",
    "for rxn in model.reactions:\n",
    "    lb = rxn.lower_bound\n",
    "    ub = rxn.upper_bound\n",
    "    if lb == ub:\n",
    "        model.remove_reactions([rxn])\n",
    "        print(rxn.id)\n",
    "        \n",
    "for rxn in model.reactions:\n",
    "    lb = rxn.lower_bound\n",
    "    ub = rxn.upper_bound\n",
    "    if lb == ub:\n",
    "        model.remove_reactions([rxn])\n",
    "        print(rxn.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b72cc3",
   "metadata": {},
   "source": [
    "##  Section 2 \n",
    "---\n",
    "\n",
    "This section setups and runs optgp_sampler. Parameters to define are:\n",
    "- Carbon uptake rate (C-mmol gDW-1 h-1)\n",
    "- The flux ration FALDHpp / EX_ch4_e used in methane simulations\n",
    "- The number of samples in each condition\n",
    "- The number of parallel workers to use in each indepent sampling\n",
    "\n",
    "optgp_sampler uses a Hit-and-Run sampling algorithm; therefore, results won't be exactly the same between independent runs.\n",
    "Increasing the number of samples increases the time required for the simulations. For reference, 10,000 samples for the four conditions took ~10min in a Ryzen 7 gen 2 using 4 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd7bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used in Methane simulations\n",
    "carbon_rate = 3.5\n",
    "faldhpp = 0.2\n",
    "\n",
    "# Parameters used in all simulations\n",
    "samples = 1000\n",
    "parallel_workers=4\n",
    "\n",
    "cmol_by_mol = 3\n",
    "valid_samples = {}\n",
    "\n",
    "# Open a model instance to make changes reversible\n",
    "with model as instance:\n",
    "    \n",
    "    # Set the Carbon uptake rate\n",
    "    medium = instance.medium\n",
    "    medium[\"EX_ch4_e\"] = carbon_rate\n",
    "    instance.medium = medium\n",
    "    \n",
    "    # Constraint ratio FALDHpp / EX_ch4_e\n",
    "    instance.reactions.FALDHpp.lower_bound = faldhpp * carbon_rate\n",
    "    \n",
    "    # Flux sampling\n",
    "    optgp_sampler = OptGPSampler(instance, processes=parallel_workers)\n",
    "    samples_ch4 = optgp_sampler.sample(samples)\n",
    "    \n",
    "    # Recover only valid samples (mass balanced samples)\n",
    "    valid_samples[\"Methane\"] = samples_ch4[optgp_sampler.validate(samples_ch4) == \"v\"]\n",
    "\n",
    "with model as instance:\n",
    "    \n",
    "    # Set the Carbon uptake rate\n",
    "    medium = instance.medium\n",
    "    substrate_rate = carbon_rate / cmol_by_mol\n",
    "    medium['EX_c3h8_e'] = substrate_rate\n",
    "    instance.medium = medium\n",
    "\n",
    "    # Flux sampling\n",
    "    optgp_sampler = OptGPSampler(instance, processes=parallel_workers)\n",
    "    samples_c3h8 = optgp_sampler.sample(samples)\n",
    "    \n",
    "    # Recover only valid samples (mass balanced samples)\n",
    "    valid_samples[\"Propane\"] = samples_c3h8[optgp_sampler.validate(samples_c3h8) == \"v\"]\n",
    "\n",
    "with model as instance:\n",
    "    \n",
    "    # Set the Carbon uptake rate\n",
    "    medium = instance.medium\n",
    "    substrate_rate = carbon_rate / cmol_by_mol\n",
    "    medium['EX_2ppoh_e'] = substrate_rate\n",
    "    instance.medium = medium\n",
    "\n",
    "    # Flux sampling\n",
    "    optgp_sampler = OptGPSampler(instance, processes=parallel_workers)\n",
    "    samples_ppoh = optgp_sampler.sample(samples)\n",
    "    \n",
    "    # Recover only valid samples (mass balanced samples)\n",
    "    valid_samples[\"Isopropanol\"] = samples_ppoh[optgp_sampler.validate(samples_ppoh) == \"v\"]\n",
    "\n",
    "with model as instance:\n",
    "    \n",
    "    # Set the Carbon uptake rate\n",
    "    medium = instance.medium\n",
    "    substrate_rate = carbon_rate / cmol_by_mol\n",
    "    medium['EX_acetone_e'] = substrate_rate\n",
    "    instance.medium = medium\n",
    "\n",
    "    # Flux sampling\n",
    "    optgp_sampler = OptGPSampler(instance, processes=parallel_workers)\n",
    "    samples_acetone = optgp_sampler.sample(samples)\n",
    "    \n",
    "    # Recover only valid samples (mass balanced samples)\n",
    "    valid_samples[\"Acetone\"] = samples_acetone[optgp_sampler.validate(samples_acetone) == \"v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcd386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell calculates median values for each reaction, the log2 foldchange for each condition using methane\n",
    "# as a reference, and summary statistics\n",
    "\n",
    "summary_tests = {}\n",
    "directionality = []\n",
    "writer = pd.ExcelWriter('data_files/summary_sampling.xlsx')\n",
    "\n",
    "for substrate in [\"Propane\", \"Isopropanol\", \"Acetone\"]:\n",
    "    \n",
    "    summary_tests[substrate] = pd.DataFrame()\n",
    "    \n",
    "    for id in valid_samples[\"Methane\"].columns:\n",
    "        if \"EX_ch4_e\" not in id:\n",
    "            \n",
    "            # Calculate median value for each reaction\n",
    "            median_ch4 = np.median(valid_samples[\"Methane\"].loc[:, id])\n",
    "            median_substrate = np.median(valid_samples[substrate].loc[:, id])\n",
    "            \n",
    "            # Calculate the flux foldchange\n",
    "            # When reactions have the same directionality\n",
    "            if abs(median_substrate * median_ch4) > 0:\n",
    "                foldchange = median_substrate / median_ch4\n",
    "                \n",
    "            # When reactions change directionality\n",
    "            elif abs(median_substrate * median_ch4) < 0:\n",
    "                directionality.append(id)\n",
    "                foldchange = median_substrate / median_ch4\n",
    "                \n",
    "            # When reaction flux is zero using Methane\n",
    "            elif abs(median_substrate) > 0:\n",
    "                foldchange = 2\n",
    "                \n",
    "            # When reaction flux is zero using 'Substrate'\n",
    "            else:\n",
    "                foldchange = 0.5\n",
    "            \n",
    "            # For reactions with the same directionality\n",
    "            if foldchange < 0:\n",
    "                directionality.append(id)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Calculate the log2 foldchange\n",
    "                log2F = np.log2(foldchange)\n",
    "                \n",
    "                # Calculate summary statistics and tests values\n",
    "                wasserstein = stats.wasserstein_distance(\n",
    "                    valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id]\n",
    "                )\n",
    "                energy = stats.energy_distance(\n",
    "                    valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id]\n",
    "                )\n",
    "                moods = stats.ks_2samp(\n",
    "                    valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id]\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    kruskal = stats.kruskal(\n",
    "                        valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id]\n",
    "                    )\n",
    "                    summary_tests[substrate].loc[id, \"kruskal\"] = kruskal.statistic\n",
    "                except ValueError:\n",
    "                    summary_tests[substrate].loc[id, \"kruskal\"] = 0\n",
    "                    continue\n",
    "                    \n",
    "                ttest = stats.ttest_ind(\n",
    "                    valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id], equal_var=False\n",
    "                )\n",
    "                kstest = stats.ks_2samp(\n",
    "                    valid_samples[substrate].loc[:, id], valid_samples[\"Methane\"].loc[:, id]\n",
    "                )\n",
    "\n",
    "                # Save the results\n",
    "                summary_tests[substrate].loc[id, \"median_ch4\"] = median_ch4\n",
    "                summary_tests[substrate].loc[id, \"median\"] = median_substrate\n",
    "                summary_tests[substrate].loc[id, \"log2F\"] = log2F\n",
    "                summary_tests[substrate].loc[id, \"wasserstein\"] = wasserstein\n",
    "                summary_tests[substrate].loc[id, \"energy\"] = energy\n",
    "                summary_tests[substrate].loc[id, \"moods\"] = moods[0]\n",
    "                summary_tests[substrate].loc[id, \"kruskal\"] = kruskal.statistic\n",
    "                summary_tests[substrate].loc[id, \"ttest\"] = ttest.statistic\n",
    "                summary_tests[substrate].loc[id, \"kstest\"] = kstest.statistic\n",
    "                \n",
    "    summary_tests[substrate].to_excel(writer, sheet_name=substrate)\n",
    "    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ba987",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "\n",
    "For each simulation calculate a Mass Flow Graph (MFG). MFGs are wighted directed networks in which nodes represent reactions, edges represent supplier-consumer relationships between reactions, and weights given by the mass flow between connected reactions. PageRank values are an indicator of the centrality of the reactions that intagrates the connectivity and the mass flow.\n",
    "\n",
    "**Important note: this is the most computationally intesive section of the notebook. For reference, calculating PageRank values for 10,000 simulations for the 4 conditions took ~4hrs in a Ryzen 7 gen 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fb95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of probabilities (0.9984202211690364) below tolerance level 0.001\n",
      "Remove blocked metabolites and reactions from the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [05:36<00:00,  2.97it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [05:41<00:00,  2.93it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [05:44<00:00,  2.90it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [05:54<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "workers = 4\n",
    "chunksize = 1\n",
    "\n",
    "# The method used to calculate MFG discriminates between forward and reversible reactions,\n",
    "# Here we create a list of forward and reversible reactions used for indexing of the results\n",
    "\n",
    "r_f = []\n",
    "r_r = []\n",
    "for rxn in model.reactions:\n",
    "    r_f.append(rxn.id)\n",
    "    r_r.append(rxn.id + \"_r\")\n",
    "\n",
    "rxn_id = r_f.copy()\n",
    "rxn_id.extend(r_r)\n",
    "\n",
    "# Create a model instance to make all changes reversible\n",
    "with model as instance:\n",
    "    \n",
    "    # Pre-process the model\n",
    "    medium = instance.medium\n",
    "    medium[\"EX_ch4_e\"] = carbon_rate\n",
    "    medium['EX_c3h8_e'] = substrate_rate\n",
    "    medium['EX_2ppoh_e'] = substrate_rate\n",
    "    medium['EX_acetone_e'] = substrate_rate\n",
    "    instance.medium = medium\n",
    "    m = len(instance.reactions)\n",
    "    \n",
    "    # Calculate the normalized flow graph of the model, and matrices S2m_p, and S2m_c used\n",
    "    # as input to the function calculating MFGs and PageRank values\n",
    "    NFG, S2m_p, S2m_c = build_normilized_flow_graph(instance, tol=1e-3)\n",
    "\n",
    "    # Set up the list of inputs as required by the parallel processing package\n",
    "    # Each element of the list contains: (Solution[i], S2m_p, S2m_c)\n",
    "    items = [(valid_samples[\"Methane\"].loc[s, :].to_numpy(), S2m_p, S2m_c) for s in valid_samples[\"Methane\"].index]\n",
    "    \n",
    "    pagerank_ch4 = pd.DataFrame(columns=rxn_id)\n",
    "    with Pool(workers) as pool:\n",
    "        with tqdm(total=samples) as pbar:\n",
    "            \n",
    "            # Parallel processing of each solution\n",
    "            for pagerank in pool.imap_unordered(build_mass_flow_graph, items, chunksize=chunksize):\n",
    "                pagerank.columns = rxn_id\n",
    "                pagerank_ch4 = pagerank_ch4.append(pagerank)\n",
    "                pbar.update()\n",
    "    pagerank_ch4.to_csv(\"data_files/pagerank_Methane.csv\")\n",
    "    \n",
    "    # Set up the list of inputs as required by the parallel processing package\n",
    "    # Each element of the list contains: (Solution[i], S2m_p, S2m_c)\n",
    "    items = [(valid_samples[\"Propane\"].loc[s, :].to_numpy(), S2m_p, S2m_c) for s in valid_samples[\"Propane\"].index]\n",
    "\n",
    "    pagerank_propane = pd.DataFrame(columns=rxn_id)\n",
    "    with Pool(workers) as pool:\n",
    "        with tqdm(total=samples) as pbar:\n",
    "            \n",
    "            # Parallel processing of each solution\n",
    "            for pagerank in pool.imap_unordered(build_mass_flow_graph, items, chunksize=chunksize):\n",
    "                pagerank.columns = rxn_id\n",
    "                pagerank_propane = pagerank_propane.append(pagerank)\n",
    "                pbar.update()\n",
    "    pagerank_propane.to_csv(\"data_files/pagerank_Propane.csv\")\n",
    "\n",
    "    # Set up the list of inputs as required by the parallel processing package\n",
    "    # Each element of the list contains: (Solution[i], S2m_p, S2m_c)\n",
    "    items = [(valid_samples[\"Isopropanol\"].loc[s, :].to_numpy(), S2m_p, S2m_c) for s in valid_samples[\"Isopropanol\"].index]\n",
    "    \n",
    "    pagerank_propanol = pd.DataFrame(columns=rxn_id)\n",
    "    with Pool(workers) as pool:\n",
    "        with tqdm(total=samples) as pbar:\n",
    "            \n",
    "            # Parallel processing of each solution\n",
    "            for pagerank in pool.imap_unordered(build_mass_flow_graph, items, chunksize=chunksize):\n",
    "                pagerank.columns = rxn_id\n",
    "                pagerank_propanol = pagerank_propanol.append(pagerank)\n",
    "                pbar.update()\n",
    "    pagerank_propanol.to_csv(\"data_files/pagerank_Isopropanol.csv\")\n",
    "\n",
    "    # Set up the list of inputs as required by the parallel processing package\n",
    "    # Each element of the list contains: (Solution[i], S2m_p, S2m_c)\n",
    "    items = [(valid_samples[\"Acetone\"].loc[s, :].to_numpy(), S2m_p, S2m_c) for s in valid_samples[\"Acetone\"].index]\n",
    "\n",
    "    pagerank_acetone = pd.DataFrame(columns=rxn_id)\n",
    "    with Pool(workers) as pool:\n",
    "        with tqdm(total=samples) as pbar:\n",
    "            \n",
    "            # Parallel processing of each solution\n",
    "            for pagerank in pool.imap_unordered(build_mass_flow_graph, items, chunksize=chunksize):\n",
    "                pagerank.columns = rxn_id\n",
    "                pagerank_acetone = pagerank_acetone.append(pagerank)\n",
    "                pbar.update()\n",
    "    pagerank_acetone.to_csv(\"data_files/pagerank_Acetone.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
